{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on making this far. It is wonderful to see your effort. Usually, when you receive a dataset or raw data from the source like JSON, SQL Databases, excel or csv or various sources. You need to first understand where the data is coming from. Who have collected these data, what was the criteria upon which these data was collected. Was it for internal research purpose, was it for decision making, was it for to understand how their product will perform in the past and to predict the same for the future. \n",
    "\n",
    "\n",
    "No two dataset are different, s care must be done to ake sure we know the data to work on this. To do this, we have a process called Exploratory Data Analysis (EDA), you are like a crew of the Starship Enterprise who job is to explore the unknown and go where no one gone before. It will be a long and perilious journey, it will be filled with grief, confusion and happiness and it is all part of the journey in EDA.\n",
    "\n",
    "Warm engines are primed captain, dock clampings are released and we need permission to be on the underway. \n",
    "\n",
    "For that you will say, Enagage, like a true starship captain Picard or kirk. Be it is your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose we do EDA is to understand the data we have, to fix the what's missing and find a most suitable algorithm to apply to this. \n",
    "\n",
    "Generally EDA is done in a step by step way to be efficient and save time, by reading many articles, we have concluded to a decision that upto 50 - 60% of the time on project will be spent on the EDA. \n",
    "\n",
    "    Here are the steps which will be extremly helpful, if you come across the dataset.\n",
    "    \n",
    "        * Step - 1: Always understand for which business or industry you are working your dataset on. DO research on this, if it for BFSI understand the business domain knowledge, if it is for automobiles, understand the company values, their vision. This will be extremly helpful when you are presenting your data to the team.\n",
    "        \n",
    "        * Step - 2: On what basis this data has been collected, for BFSI, is for credit card department, is it for Loan. understand the business accumen. Have questions, best bet is to google or still unable to understand something, reach out to apprpriate team for assistance.\n",
    "        \n",
    "        * Step - 3: Now you have crossed the first two steps with ease, now will be the time, you start handling data. First and foremost, understand which file format is your data in, is it excel, csv, word format, pdf, databses and so on. Once this has been decided, load the dataset to the IDE you choose, it may be Jupyter notebook, Spyder, Google Colab, and so on. Attention must be paid so that you have atleast 8GB and above RAM to handle this load of data. \n",
    "        \n",
    "        * Step - 4: Data has been loaded to a variable, decide on dataframe name or dataseries. Most will use pandas library for this task. Then you will set max rows and max columns to display and you will check to see if all the features(column name) is present as been provided by the data provider. If they say, you will have 25 features, you need to check and see if all present. Any missing, you need to reach out to data provider and see if they have sent proper file or you have received the most updated information. \n",
    "        \n",
    "        * Step - 5: Check for null values, null values are the values which are missing something or the information has not be provided and you shall replace them with a new value which can be abc, xyz for the object type data (or data which can be like a word, for example, say, in gender, you have three options, Male, female, do not wish to enclose. Sometimes, null will be no information collected on this. Null values affects the outcome and sometimes be considered as an outlier, to avoid this, we will say a new name for all null values, in case of numerical(int or float), we will perform mean or median function and replace the numerical null values). Now you have handled the null value. It is refereed as Missing value imputation.\n",
    "        \n",
    "        * Step - 6: Now you perform some analysis: uni-variate, Bi-Variate analysis. If need multi-variate analysis.\n",
    "            You will select a feature and check it dependency or how it influence the target variable.\n",
    "            \n",
    "        * Step - 7: Between step 4 and step 5, you will do perform a task known as Outlier treatment. This will help you to drop some values which is too low or too high outside the norm.\n",
    "        \n",
    "        * Step - 8: Now, we perform correlation analysis, where you see how a features with numerical values co-relates with each other. Here decision will be made to drop a feature or whether we need more data for further analysis.\n",
    "        \n",
    "        * Step - 9: Now that we have handled quantitative features, we shall turn our focus on qualitative features. These are referred as categorical feattures. We will need to do tranformation, like OneHotEncoding, Binary Encoding, Hash Encoding, Dummy Variable encoding, Label or ordinal encoding. Based on the counts of the categrical data, we decide on the best encoding which will help you decide on best Machine Learing algorithm.  \n",
    "        \n",
    "        * Step - 10: Now that we have decided on the features, done with most basic EDA, also, upon plotting the features with each other, we have decided on ML algorithm. Now, we will do one final thing which would be to a train-test split where train will use your data to teach a model on how to work and test will choose it's own data to check whether the model you have trained is working correctly.\n",
    "        \n",
    "        * Step - 11: In case, you have used neural networks in your model, you will do train-validation-test split instead of train and test because, validation will be used for fine tuning the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Pictorial Representation of the EDA Steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![]('l1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](l1.png)\n",
    "\n",
    "![](l2.png)\n",
    "\n",
    "![](l3.png)\n",
    "\n",
    "![](l4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This sounds like a lot, way more than you ever hoped. Worry not. After seeing the skeleton key for the regression, classification, clustering. We will understand the same pretty well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
