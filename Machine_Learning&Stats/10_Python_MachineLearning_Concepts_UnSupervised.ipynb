{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. In this unsupervised ML, we will learn the following :\n",
    "    \n",
    "        1. K-Means Clustering\n",
    "        2. Hierarchial Clustering\n",
    "        3. Agglomerative Hierarchial Clustering\n",
    "        4. Density Based Clustering (DBSCAN)\n",
    "        5. Divisive Hierarchial Clustering\n",
    "        \n",
    "        \n",
    "B. Plan : This notebook page will have a brief explaination on each method. There will be some images for visual representation \n",
    "    which will crude. However, while working with a kaggle dataset, I will try to explain the visualisation as best to my abilities.\n",
    "    \n",
    "C. We will pick up pace and will do multiple Unsupervised ML with two kaggle dataset. I will try my best to explain this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following example.\n",
    "\n",
    "You have three types of berries, Raspberries, Blueberries, Strawberries. You have mixed these successfully and now you ask your friend to seperate these. It will be laborus process. You can spend as little time to seperate blueberries since they are out of colors. Now you have hard time seperating reds since raspberries and strawberries based on color. Rememer, strawberries are bigger than raspberries. you are seperating based on color because a computer can be trained to check color and not size since the computer doesn't have ability to handle physical aspect.\n",
    "\n",
    "So you will use clustering here, you will cluster the whole red color set into say 10 groups, cluster will then based on parameters will find a center of the cluster, then will begin sorting this out. For us humans, this is found strange and in efficient, in small samples, but larger sample have more success with this process. For PC, this is a larger process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is the task of grouping together a set of objects in a way that objects in the same cluster are more similar to each other than to objects in other clusters. Similarity is a metric that reflects the strength of relationship between two data objects. Clustering is mainly used for exploratory data mining. It has manifold usage in many fields such as machine learning, pattern recognition, image analysis, information retrieval, bio-informatics, data compression, and computer graphics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. K-Means Clustering\n",
    "\n",
    "* helps gain insight from unlabelled data.\n",
    "* Widely used to handle unlabelled data due to it's simplicity and performance.\n",
    "* K-means algorithm partition n observations into k clusters where each observation belongs to the cluster with the nearest mean serving as a prototype of the cluster .\n",
    "* You’ll define a target number k, which refers to thenumber of centroids you need in the dataset. A centroid is the imaginary or real location representing the centre of the cluster.In other words, the K-means algorithm identifies k number of centroids, and then allocatesevery data point to the nearest cluster, while keeping the centroids as small as possible.The ‘means’ in the K-means refers to averaging ofthe data; that is, finding the centroid.\n",
    "* K-means is a centroid-based algorithm, or a distance-based algorithm, where we calculate the distances to assign a point to a cluster. In K-Means, each cluster is associated with a centroid. The main objective of the K-Means algorithm is to minimize the sum of distances between the points and their respective cluster centroid.\n",
    "\n",
    "Steps for K-Means Clustering ops:\n",
    "\n",
    "    Step - 1: First datapoints are plotted on x-y axiss\n",
    "    Step - 2: Two random points are selected on the figure based on crude grouping and it will be the centroid of those grouping.\n",
    "    Step - 3: The data point nearest to the centroid will be alloted a grouping value or cluster value such as cluster-1, cluster-2 and so on\n",
    "    Step - 4: Now, once the grouping is successful,the newest centroid will be re-selected. Another successful grouping will take place.\n",
    "    Step - 5: The iterative process of Step 2,3,4 are carried out until the computer cannot learn new things or same results keeps popping consecutively, we conclude the clustering is complete and has been successfully carried out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/kmcluster_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hierarchial Clustering or Hierarchial Clustering Analysis (HCA)\n",
    "## 3. Agglomerative Hierarchial Clustering\n",
    "## 4. Divise  Hierarchial Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With two main drawbcks from K-means clustering, we will use a better alogorith Hierarchial Clustering.\n",
    "* \n",
    "    * Drawback - 1: K-means always need to specify the number of cluster.\n",
    "    * Drawback - 2: The centroid are usually selected at random, therefore, in terms of iterative process, it varies, so it tries to get cluster of same sizes. \n",
    "    \n",
    "* To handle these drawback, we will end up using Hierarchial Clustering where the clustering is represented by te tree-shaped Dendogram. \n",
    "\n",
    "* The Bottom-up approach, where the algorithm starts with taking all data points as single clusters and merging them until one cluster in left. This is also known as Agglomerative Hierarchial Clustering.\n",
    "\n",
    "* The Top-Down approach, where the algorithm starts with taking a single cluster and keep spreading until each cluster have single data points, if you have say 10K data points, 10K clustering can be expected. This is also known as Divisive Hierarchial Clustering.\n",
    "\n",
    "See the examples below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchial Clustering - Agglomerative\n",
    "![](images/Hierarchial_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchial Clustering - Divise\n",
    "![](images/Hierarchial_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Density Based Clustering(DBSCAN)\n",
    "\n",
    "* Most of the traditional clustering techniques, such as k-means, hierarchical and fuzzy clustering, can be used to group data without supervision.\n",
    "\n",
    "* However, when applied to tasks with arbitrary shape clusters, or clusters within cluster, the traditional techniques might be unable to achieve good results. That is, elements in the same cluster might not share enough similarity or the performance may be poor. Additionally, Density-based Clustering locates regions of high density that are separated from one another by regions of low density. Density, in this context, is defined as the number of points within a specified radius.\n",
    "\n",
    "* In this part, the main focus will be manipulating the data and properties of DBSCAN and observing the resulting clustering.\n",
    "\n",
    "* Modeling\n",
    "\n",
    "    * DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. This technique is one of the most common clustering algorithms which works based on density of object. The whole idea is that if a particular point belongs to a cluster, it should be near to lots of other points in that cluster.\n",
    "\n",
    "    * It works based on two parameters: Epsilon and Minimum Points\n",
    "\n",
    "        * Epsilon determine a specified radius that if includes enough number of points within, we call it dense area\n",
    "        * minimumSamples determine the minimum number of data points we want in a neighborhood to define a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
