{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "    * Regression is a process of finding the correlation between dependent and independent variable. It helps predicting the continious variables such as predictions, like market trends, house pricing etc\n",
    "\n",
    "    * It is very much important to find the mapping function to map the input variable (x) to the continious output variable (y).\n",
    "\n",
    "    * As x changes, y is impacted heavily, this is why x is always determined as independent variable and y as dependent variable.\n",
    "    \n",
    "    * Types of Regression Algorithm:\n",
    "\n",
    "    * Simple Linear Regression\n",
    "    * Multiple Linear Regression\n",
    "    * Polynomial Regression\n",
    "    * Support Vector Regression\n",
    "    * Decision Tree Regression\n",
    "    * Random Forest Regression\n",
    "    \n",
    "    \n",
    "###### Here you will find the skeleton and a simple explaination. However, while we work on dataset, effort will be made to show the same with an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression \n",
    "\n",
    "    * Before we proceed, let us first understand what is Linear Regression and what is it we are looking for it.\n",
    "    \n",
    "    * In regression analysis, independent variable is referred as regressor or predictor and the dependent variable is often referred as regressed or explained variable. \n",
    "    \n",
    "    * It is know that a linear regression, with one dependent variable (y) and one independent variable (x) can be expressed as below : \n",
    "    \n",
    "            y = mx + b\n",
    "            \n",
    "                where, m is the slope of the line and \n",
    "                       b is the y intercept of the line. \n",
    "                       \n",
    "    * However, there are always random error present in the regression analysis and it is denoted by epsilon. Then linear regression is expressed as below :\n",
    "    \n",
    "            y = mx + b + e\n",
    "            \n",
    "                 where, e is the epsilon and the fitted points outside the regression line.\n",
    "                 \n",
    "                 \n",
    "                 \n",
    "    * Fitted points are the plotted points on the x-y graph. Not all points are used to plot the regression line. \n",
    "    \n",
    "    * A note must be made that, there is no deterministic linear regression model since it is an ideal condition, which means all the plotted points form a regression line, hence we will alway assume there will be error and choose probablilistic model.\n",
    "    \n",
    "![](images/simlinreg_1.jfif)\n",
    "    \n",
    "\n",
    "\n",
    "####  4 properties for regression analysis\n",
    "\n",
    "1. The regression line minimized the sum of square difference between observed value and predicted value.\n",
    "2. The regression line should pass through mean of x and mean of y as a co-ordinate.\n",
    "3. The regression constant i.e b should always be equal to y-intercept which is above or below the origin.\n",
    "4. The regresion co-efficient is the average change in the dependent variable for a unit change in the independent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of regression analysis, technique and fctors.\n",
    "\n",
    "1. Types of target variables.\n",
    "2. Shape of regression line.\n",
    "3. Number of independent variables.\n",
    "\n",
    "Types of regression analysis,\n",
    "\n",
    "1. Linear model - Linear Regression, Multiple Linear Regression\n",
    "2. Non-Linear model - Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step - 0: import necessary packages and class (pandas, numpy, matplotlib, seaborn and so on)\n",
    "\n",
    "Step - 1: import data from source and create dependent and independent features.\n",
    "\n",
    "Step - 2: import linear regression library from sklearn\n",
    "\n",
    "            from sklearn.linear_model import Linear Regression as LR\n",
    "            \n",
    "Step - 3: create a train and test split using sklearn model selection library\n",
    "\n",
    "            from sklearn.model_selection import train_test_split as ttst\n",
    "and proceed with splitting the dataset into train and test set\n",
    "\n",
    "            x_train, x_test, y_train, y_test = ttst(x, y, test_size = 0.3, random_state = 0) \n",
    "where test_size = 0.33 means 33% of the dataset will be reserved for testing the linear model(in this case) based on the training set which will be carried out using your 66%. Always keep in mind that, training set should be larger above 65%, the larger the training set, the better you will train the model. \n",
    "\n",
    "Step - 4:  creating model\n",
    "\n",
    "            model = LR() \n",
    "where you will create an instance of the class Linear Regression. Once this done,we will proceed to train the model with x_train and y_train.\n",
    "\n",
    "            model.fit(x_train, y_train) \n",
    ".fit() will perform linear regression anaysis on the x_train and y_train subsets.\n",
    "\n",
    "Step - 5: Now, we get result to see whether our model has worked as we have intended by finding inercept and co-efficient. This step is optional, but since we are beginners, we will use this.\n",
    "\n",
    "            print('Intercept of the regression line is : ', model.intercept_)\n",
    "            print('Slope of the regression line is : ', model.coef_)\n",
    "            \n",
    "Step - 6: Now, we are satisfied with the model training, let us move towards prediction.\n",
    "\n",
    "            ytrain_pred = model.predict(x_train)\n",
    "            \n",
    "where ytrain_pred a new variable will now predict the value of y using the x_train subset. This will see whether the trained model shows a value near or near to values of y and predicted y_value. Now we will predict the value of test set using x_test subset. Simply put, we will see how will the model react to the x_test subset as if the same response is expected during predicting for new values.\n",
    "\n",
    "            ytest_pred = model.predict(x_test)\n",
    "            \n",
    "Step - 7: Now, find the relationship between the x and y. we will perform a coefficient of correlation  also know as r value. Complete correlation between two variables is expressed by either + 1 or -1. When one variable increases as the other increases the correlation is positive; when one decreases as the other increases it is negative. If r value is 0, then there is no relationship between x and y.\n",
    "\n",
    "            r_sqr = model.score(xtrain, y_train)\n",
    "            print('R value is: ', r_sqr) \n",
    "            \n",
    "If R value is 1, this an absolute fit which doesn't exist, if R value is -1, this means model is not performing accurately and have outliers present. A good model will have between 0 and 0.9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Linear regression model is explained in notebook 9B.A_Python_Regression_LinearRegression and all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression or SVR\n",
    "\n",
    "Consider an x,y plot, now we will successfully plotted a regression line by assuming the dataset is linear. You will see that there are many random error points. The more data points, the better training model. Let us now, see how it will be if we are choose support vector method for the regression analysis.\n",
    "\n",
    "Refer the below figure :: \n",
    "\n",
    "You will see that the linear method has too many random error and we will need a better analysis model - (A), what if we were to choose a support vector to minimize random error points - (B). \n",
    "\n",
    "Before we proceed, let us understand one concept, Ordinary Least Squares, The \"least squares\" method is a form of mathematical regression analysis used to determine the line of best fit for a set of data, providing a visual demonstration of the relationship between the data points. Each point of data represents the relationship between a known independent variable and an unknown dependent variable. \n",
    "\n",
    "The least squares method provides the overall rationale for the placement of the line of best fit among the data points being studied. The most common application of this method, which is sometimes referred to as \"linear\" or \"ordinary\", aims to create a straight line that minimizes the sum of the squares of the errors that are generated by the results of the associated equations, such as the squared residuals resulting from differences in the observed value, and the value anticipated, based on that model.\n",
    "\n",
    "Coming back to SVR,as seen below, we have a tube with regression line accompanied by two support vectors on either side known as epsilon insensitive tube. That means any data points inside this tube is allowed and considered as margin or error. Any data points outside the tube will be the error points represented epsilon i star for below data points and epsilon i for above data points and referred as slack varables and is measured between the tube and the error point rather than regression line and error point.\n",
    "\n",
    "![](images/svr_1.jfif)\n",
    "\n",
    "Since, the slack variables are supporting the regression analysis and it is represented as 2-D vector, it is known as Support Vector analysis. \n",
    "\n",
    "See 9B.B_Python_Regression_SVR,Decision Tree and Random Forest Regression for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression and Intuition\n",
    "\n",
    "* CART concept - Classification And Regression Tree concept. \n",
    "\n",
    "Decision Tree is based on the concept of selecting a node and then plotting a leaf node based on decision yes or no. And based on the number of features in your dataset, the nodes attributes are created at leaf node and subsequently so on. \n",
    "\n",
    "Before we proceed, let us understand the concept of Decision Tree entrophy. \n",
    "\n",
    "![](images/decision_1.jfif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Technique and Random Forest CART\n",
    "\n",
    "Before we proceed with Random Forest, let us see what is Ensemble technique and it's two most used methods.\n",
    "\n",
    "1. Bagging technique or Bootstrap aggregation - Random Forest Technique\n",
    "2. Boosting - ADA Boost, XG Boost, Gradient Boosting\n",
    "\n",
    "\n",
    "1. Baggine technique : \n",
    "\n",
    "Consider a dataset named D and it is known it will have many features, many rows. Ensemble technique means many models are combined to work as one, there by increasing efficiency. Let the many models be M1, M2, M3, .... Mk and we will provide a row sample of dataset D referred as Dm dash, dataset D has n number of values. Note that m < n. For each model, we will resmaple and it is called as Row Samplings with replacement. This ensemble methods is used to perform model training.\n",
    "\n",
    "You introuduce a new dataset Dt and you introuduce this to the ensmble and you will try to predict the outcome. If your each model will provide a value of 0 or 1, based on majority of voting, we will give the final value as 1.\n",
    "\n",
    "Bootstrap is when we re-sample the one dataset to many models and aggregation is when based on the assumed many models outcome, we will select a value based on majority.\n",
    "\n",
    "![](images/bagging_1.png)\n",
    "\n",
    "\n",
    "\n",
    "### Random Forest with decision Tree\n",
    "\n",
    "* A dataset D, with many base learning model, M1, M2, M3, .... Mn and each learning model are decision tree referred as DT. \n",
    "The dataset D has 'd' number of records(rows) and 'm' number of features(columns). Select some sample or rows and some sample of features with the help of Row Samplings(RS) and Feature samplings(FS) to feed to DT1 in M1 model and sample the \n",
    "RS + FS be named as D1 dash. \n",
    "\n",
    "![](images/randomforest_1.png)\n",
    "\n",
    "\n",
    "* Decision Tree is based on the condition that it has  A.Low bias and B.High Variance.\n",
    "\n",
    "* Low Bias means if we are to creating decision tree analysis to the complete depth, we will train our decision tree properly on training dataset and training error will be very less.\n",
    "\n",
    "* High Variance says that when we get new test data, the decision tree which has been trained will be prone to give larger amount of errors.\n",
    "\n",
    "* If we are to base decision on decision tree, it will leads to overfitting and random forest it will perform analysis on number of decision trees and perform aggregation on majority of the votes, it will leads to lower variance means very lesser errors.\n",
    "\n",
    "\n",
    "* In Random Forest regression it will find the mean or median of the particular output and in case of Random Forest classification, it will find the majority of the votes based on the output of number of models considered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
