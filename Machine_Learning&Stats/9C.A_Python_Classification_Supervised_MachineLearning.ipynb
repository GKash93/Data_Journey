{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "1. What is classification?\n",
    "2. Why do we need classification?\n",
    "3. Types of classification and the learners\n",
    "4. Theoretical concept and coding example \n",
    "\n",
    "### 1. What is classification in supervised machine learning?\n",
    "\n",
    "Classification in ML is used to identify the category of new observations on the basis of training data. In classification, the program learns from the given dataset or observations and then classifies new observation into a number of classes or groups. Classes can be called as targets/labels or categories.\n",
    "\n",
    "Classification primary deals with categorical data rather than numerical data. Which helps in projects like identifying an object, spam detection and so on.\n",
    "\n",
    "### 2. Why do we need classification ?\n",
    "\n",
    "In a given dataset, chances are that the there are more categorical features rather than numerical data. If there exists a relation between the categorical features and the output features, regression analysis will perform to fail or will fail. Classification will be best used in this case. \n",
    "\n",
    "In classification algorithm, a discrete output function, y is mapped to input variables (x).\n",
    "\n",
    "y = f(x), where y = categorical output.\n",
    "\n",
    "### 3. Types of classification and the learners \n",
    "\n",
    "    A. Binary Classifiers - Based on the binary outcomes problems, we use binary classifiers. Examples - Gender based analysis, cat or dog, Decision based like Yes or no, and so on. Please note that gender based analysis can also be implemented in multi-class classifiers.\n",
    "    \n",
    "    B. Multi-class classifiers - If a classification problem has more than two outcomes, it is known as multi-class classifiers. Examples - classification of types of types of crops, classification of musics.\n",
    "    \n",
    "#### Learners \n",
    "    \n",
    "    1. Lazy Learners: Lazy Learner firstly stores the training dataset and wait until it receives the test dataset. In Lazy learner case, classification is done on the basis of the most related data stored in the training dataset. It takes less time in training but more time for predictions.\n",
    "        Example: K-NN algorithm, Case-based reasoning\n",
    "        \n",
    "    2. Eager Learners: Eager Learners develop a classification model based on a training dataset before receiving a test dataset. Opposite to Lazy learners, Eager learners take less time in training and more time in prediction. Example: Decision Trees, Na√Øve Bayes.\n",
    "    \n",
    "    \n",
    "### What will we learn in ML classification : \n",
    "\n",
    "    * Logistic Regression\n",
    "    * K-Nearest Neighbour (K-NN)\n",
    "    * Support Vector Machine (SVM)\n",
    "    * Kernel - SVM\n",
    "    * Naive Bayes Classifier\n",
    "    * Decision Tree Classifier\n",
    "    * Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression classifier \n",
    "\n",
    "* Logistic regression is mostly applied to linear models. In linear models, the regression line is always straight irrrespective of the origins.\n",
    "* It is used to predict the categorical dependent variables using a given set of independent variables.\n",
    "* Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.\n",
    "\n",
    "![](images/log_1.jfif)\n",
    "![](images/log_2.jfif)\n",
    "![](images/log_3.jfif)\n",
    "![](images/log_4.jfif)\n",
    "![](images/log_5.jfif)\n",
    "![](images/log_6.jfif)\n",
    "![](images/log_7.jfif)\n",
    "![](images/log_8.jfif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbour or K-NN Classifier.\n",
    "\n",
    "![](images/knn_1.jfif)\n",
    "![](images/knn_2.jfif)\n",
    "![](images/knn_3.jfif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Linear Discrimination Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Linear Discriminant Analysis is a popular technique for performing dimensionality reduction on a dataset. Dimensionality reduction is the reduction of a dataset from  n  variables to  k  variables, where the  k  variables are some combination of the  n  variables that preserves or maximizes some useful property of the dataset. In the case of Linear Discriminant Analysis, the new variables are chosen (and the data reprojected) in a way that maximizes the linear separability of a certain set of classes in the underlying data.\n",
    "\n",
    "* In other words, given a dataset with  n  variables, including an embedded set of labels that we want to predict, we can apply LDA to the data and reduce it to  k  components, where those components are chosen in such a way that they maximize our ability to \"draw lines\" to distinguish the classes.\n",
    "\n",
    "* An LDA transform is useful as a preprocessing step when modeling classes because it transforms the space in such a way that algorithms which then go and draw those boundaries, like support vector machines, perform much better on the transformed data than on the original projections. \n",
    "\n",
    "* Special Thanks to https://www.kaggle.com/residentmario/linear-discriminant-analysis-with-pokemon-stats\n",
    "\n",
    "* We will try LDA with pokemon datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines and Kernel SVM\n",
    "\n",
    "One of the best basic explaination can be seen on this webpage :: https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/svc_1.jfif)\n",
    "\n",
    "\n",
    "![](images/svc_2.jfif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier and Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If you check the notebook page, 7A_Python_StatisticalConcept_Basic, we have provided a best explaination on Bayes therory. \n",
    "\n",
    "In simple words, it will work mainly in probability and the most easiest classification to learn and understand. \n",
    "\n",
    "Three things needs t be understood here - \n",
    "\n",
    "    1. Conditional Probability\n",
    "    2. Independent Events\n",
    "    3. Dependent Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
